# remove islands with incomplete years
group_by(ISLAND) %>% filter(n_distinct(ANALYSIS_YEAR) == 4) %>% # only include islands that have 4 survey years
ungroup() #converts to standard dataframe
#options(survey.lonely.psu = "remove")
options(survey.lonely.psu = "adjust")
## define survey design
des = svydesign(id = ~1, strata = ~STRAT_CONC, weights = ~sw, data = mhi)
## replicate-weights survey design
# repdes = as.svrepdesign(des)
## calculate island/year means and SE
# standard design
isl_yearmean = svyby(~PERCENT, ~ISLAND + ANALYSIS_YEAR, design = subset(des, GROUP=="CORAL"), svymean)
library(dplyr)
library(tidyr)
library(ggplot2)
library(survey)
options(dplyr.summarise.inform = FALSE)
# read in data
cover = read.csv("C:/Users/courtney.s.couch/Documents/GitHub/December-2023-StRS-Stats-Workshop/Data/BenthicCover_SITE_analysisready.csv")
# prepare data for analysis (filtered to Main Hawaiian Islands)
mhi = cover %>% rename(SECTOR = PooledSector_Viztool) %>% #change name of column to "SECTOR"
distinct(SITEVISITID, SITE, .keep_all = TRUE) %>% #filter duplicates
mutate(NH = ifelse(is.na(NH) & DEPTH_BIN =="Mid", 50, #manually specify a NH value for one of the strata that didn't have NH
ifelse(is.na(NH),25, NH))) %>%
group_by(ANALYSIS_YEAR, SECTOR, SEC_NAME, STRATA) %>% mutate(NH = NH / n()) %>% #calculate NH (total possible sites) by year, sector, sec name and strata
group_by(ANALYSIS_YEAR, SECTOR, STRATA) %>% mutate(NH = sum(NH), n = n()) %>% #add up NHs and n (sites) for sectors that were pooled
filter(REGION == "MHI") %>% #remove strata with only 1 site and just include MHI sites
mutate(sw = NH / n, #Calculate survey weight
STRAT_CONC = paste(ANALYSIS_YEAR, REGION, ISLAND, SECTOR, STRATA, sep = "_"), #create new column with concatenated strata variable
ANALYSIS_YEAR = as.character(ANALYSIS_YEAR), #change to character
SECTOR = as.character(SECTOR), #change to character
STRATA = as.character(STRATA)) %>% #change to character
mutate(across(c(CORAL, CCA, MA, TURF), ~./100)) %>% # convert percent variables to proportions so we can use binomial
# remove islands with incomplete years
group_by(ISLAND) %>% filter(n_distinct(ANALYSIS_YEAR) == 4) %>% # only include islands that have 4 survey years
ungroup() #converts to standard dataframe
#options(survey.lonely.psu = "remove")
options(survey.lonely.psu = "adjust")
## define survey design
des = svydesign(id = ~1, strata = ~STRAT_CONC, weights = ~sw, data = mhi)
## replicate-weights survey design
# repdes = as.svrepdesign(des)
## calculate island/year means and SE
# standard design
isl_yearmean = svyby(~CORAL, ~ISLAND + ANALYSIS_YEAR, design = des, svymean)
# replicate-weights design
# isl_yearmean = svyby(~PERCENT, ~ISLAND + ANALYSIS_YEAR, design = subset(repdes, GROUP=="CORAL"), svymean)
# plot weighted mean coral cover by island and year
ggplot(isl_yearmean, aes(x = ANALYSIS_YEAR, y = PERCENT, fill = ANALYSIS_YEAR)) +
geom_bar(stat = "identity", position = position_dodge2(preserve = "single"), width = 1, color = "black") +
geom_errorbar(aes(ymin = PERCENT - se, ymax = PERCENT + se), width = 0.2) +
facet_wrap(~ISLAND, nrow = 1) +
guides(fill = "none") +
theme_bw() +
theme(panel.grid.minor = element_blank(),
panel.grid.major = element_blank(),
axis.title.x = element_blank(),
axis.title.y = element_blank(),
axis.text.x = element_text(angle = -90, hjust = 0))
library(dplyr)
library(tidyr)
library(ggplot2)
library(survey)
options(dplyr.summarise.inform = FALSE)
# read in data
cover = read.csv("C:/Users/courtney.s.couch/Documents/GitHub/December-2023-StRS-Stats-Workshop/Data/BenthicCover_SITE_analysisready.csv")
# prepare data for analysis (filtered to Main Hawaiian Islands)
mhi = cover %>% rename(SECTOR = PooledSector_Viztool) %>% #change name of column to "SECTOR"
distinct(SITEVISITID, SITE, .keep_all = TRUE) %>% #filter duplicates
mutate(NH = ifelse(is.na(NH) & DEPTH_BIN =="Mid", 50, #manually specify a NH value for one of the strata that didn't have NH
ifelse(is.na(NH),25, NH))) %>%
group_by(ANALYSIS_YEAR, SECTOR, SEC_NAME, STRATA) %>% mutate(NH = NH / n()) %>% #calculate NH (total possible sites) by year, sector, sec name and strata
group_by(ANALYSIS_YEAR, SECTOR, STRATA) %>% mutate(NH = sum(NH), n = n()) %>% #add up NHs and n (sites) for sectors that were pooled
filter(REGION == "MHI") %>% #remove strata with only 1 site and just include MHI sites
mutate(sw = NH / n, #Calculate survey weight
STRAT_CONC = paste(ANALYSIS_YEAR, REGION, ISLAND, SECTOR, STRATA, sep = "_"), #create new column with concatenated strata variable
ANALYSIS_YEAR = as.character(ANALYSIS_YEAR), #change to character
SECTOR = as.character(SECTOR), #change to character
STRATA = as.character(STRATA)) %>% #change to character
mutate(across(c(CORAL, CCA, MA, TURF), ~./100)) %>% # convert percent variables to proportions so we can use binomial
# remove islands with incomplete years
group_by(ISLAND) %>% filter(n_distinct(ANALYSIS_YEAR) == 4) %>% # only include islands that have 4 survey years
ungroup() #converts to standard dataframe
#options(survey.lonely.psu = "remove")
options(survey.lonely.psu = "adjust")
## define survey design
des = svydesign(id = ~1, strata = ~STRAT_CONC, weights = ~sw, data = mhi)
## replicate-weights survey design
# repdes = as.svrepdesign(des)
## calculate island/year means and SE
# standard design
isl_yearmean = svyby(~CORAL, ~ISLAND + ANALYSIS_YEAR, design = des, svymean)
# replicate-weights design
# isl_yearmean = svyby(~CORAL, ~ISLAND + ANALYSIS_YEAR, design = repdes, svymean)
# plot weighted mean coral cover by island and year
ggplot(isl_yearmean, aes(x = ANALYSIS_YEAR, y = CORAL, fill = ANALYSIS_YEAR)) +
geom_bar(stat = "identity", position = position_dodge2(preserve = "single"), width = 1, color = "black") +
geom_errorbar(aes(ymin = CORAL - se, ymax = CORAL + se), width = 0.2) +
facet_wrap(~ISLAND, nrow = 1) +
guides(fill = "none") +
theme_bw() +
theme(panel.grid.minor = element_blank(),
panel.grid.major = element_blank(),
axis.title.x = element_blank(),
axis.title.y = element_blank(),
axis.text.x = element_text(angle = -90, hjust = 0))
# fit Gaussian model
mod.1 = svyglm(PROP ~ ISLAND * ANALYSIS_YEAR, design = subset(des, GROUP=="CORAL"))
library(dplyr)
library(tidyr)
library(ggplot2)
library(survey)
options(dplyr.summarise.inform = FALSE)
# read in data
cover = read.csv("C:/Users/courtney.s.couch/Documents/GitHub/December-2023-StRS-Stats-Workshop/Data/BenthicCover_SITE_analysisready.csv")
# prepare data for analysis (filtered to Main Hawaiian Islands)
mhi = cover %>% rename(SECTOR = PooledSector_Viztool) %>% #change name of column to "SECTOR"
distinct(SITEVISITID, SITE, .keep_all = TRUE) %>% #filter duplicates
mutate(NH = ifelse(is.na(NH) & DEPTH_BIN =="Mid", 50, #manually specify a NH value for one of the strata that didn't have NH
ifelse(is.na(NH),25, NH))) %>%
group_by(ANALYSIS_YEAR, SECTOR, SEC_NAME, STRATA) %>% mutate(NH = NH / n()) %>% #calculate NH (total possible sites) by year, sector, sec name and strata
group_by(ANALYSIS_YEAR, SECTOR, STRATA) %>% mutate(NH = sum(NH), n = n()) %>% #add up NHs and n (sites) for sectors that were pooled
filter(REGION == "MHI") %>% #remove strata with only 1 site and just include MHI sites
mutate(sw = NH / n, #Calculate survey weight
STRAT_CONC = paste(ANALYSIS_YEAR, REGION, ISLAND, SECTOR, STRATA, sep = "_"), #create new column with concatenated strata variable
ANALYSIS_YEAR = as.character(ANALYSIS_YEAR), #change to character
SECTOR = as.character(SECTOR), #change to character
STRATA = as.character(STRATA)) %>% #change to character
mutate(across(c(CORAL, CCA, MA, TURF), ~./100)) %>% # convert percent variables to proportions so we can use binomial
# remove islands with incomplete years
group_by(ISLAND) %>% filter(n_distinct(ANALYSIS_YEAR) == 4) %>% # only include islands that have 4 survey years
ungroup() #converts to standard dataframe
#options(survey.lonely.psu = "remove")
options(survey.lonely.psu = "adjust")
## define survey design
des = svydesign(id = ~1, strata = ~STRAT_CONC, weights = ~sw, data = mhi)
## replicate-weights survey design
# repdes = as.svrepdesign(des)
## calculate island/year means and SE
# standard design
isl_yearmean = svyby(~CORAL, ~ISLAND + ANALYSIS_YEAR, design = des, svymean)
# replicate-weights design
# isl_yearmean = svyby(~CORAL, ~ISLAND + ANALYSIS_YEAR, design = repdes, svymean)
# plot weighted mean coral cover by island and year
ggplot(isl_yearmean, aes(x = ANALYSIS_YEAR, y = CORAL, fill = ANALYSIS_YEAR)) +
geom_bar(stat = "identity", position = position_dodge2(preserve = "single"), width = 1, color = "black") +
geom_errorbar(aes(ymin = CORAL - se, ymax = CORAL + se), width = 0.2) +
facet_wrap(~ISLAND, nrow = 1) +
guides(fill = "none") +
theme_bw() +
theme(panel.grid.minor = element_blank(),
panel.grid.major = element_blank(),
axis.title.x = element_blank(),
axis.title.y = element_blank(),
axis.text.x = element_text(angle = -90, hjust = 0))
# fit Gaussian model
mod.1 = svyglm(PROP ~ ISLAND * ANALYSIS_YEAR, design = subset(des, GROUP=="CORAL"))
library(dplyr)
library(tidyr)
library(ggplot2)
library(survey)
options(dplyr.summarise.inform = FALSE)
# read in data
cover = read.csv("C:/Users/courtney.s.couch/Documents/GitHub/December-2023-StRS-Stats-Workshop/Data/BenthicCover_SITE_analysisready.csv")
# prepare data for analysis (filtered to Main Hawaiian Islands)
mhi = cover %>% rename(SECTOR = PooledSector_Viztool) %>% #change name of column to "SECTOR"
distinct(SITEVISITID, SITE, .keep_all = TRUE) %>% #filter duplicates
mutate(NH = ifelse(is.na(NH) & DEPTH_BIN =="Mid", 50, #manually specify a NH value for one of the strata that didn't have NH
ifelse(is.na(NH),25, NH))) %>%
group_by(ANALYSIS_YEAR, SECTOR, SEC_NAME, STRATA) %>% mutate(NH = NH / n()) %>% #calculate NH (total possible sites) by year, sector, sec name and strata
group_by(ANALYSIS_YEAR, SECTOR, STRATA) %>% mutate(NH = sum(NH), n = n()) %>% #add up NHs and n (sites) for sectors that were pooled
filter(REGION == "MHI") %>% #remove strata with only 1 site and just include MHI sites
mutate(sw = NH / n, #Calculate survey weight
STRAT_CONC = paste(ANALYSIS_YEAR, REGION, ISLAND, SECTOR, STRATA, sep = "_"), #create new column with concatenated strata variable
ANALYSIS_YEAR = as.character(ANALYSIS_YEAR), #change to character
SECTOR = as.character(SECTOR), #change to character
STRATA = as.character(STRATA)) %>% #change to character
mutate(across(c(CORAL, CCA, MA, TURF), ~./100)) %>% # convert percent variables to proportions so we can use binomial
# remove islands with incomplete years
group_by(ISLAND) %>% filter(n_distinct(ANALYSIS_YEAR) == 4) %>% # only include islands that have 4 survey years
ungroup() #converts to standard dataframe
#options(survey.lonely.psu = "remove")
options(survey.lonely.psu = "adjust")
## define survey design
des = svydesign(id = ~1, strata = ~STRAT_CONC, weights = ~sw, data = mhi)
## replicate-weights survey design
# repdes = as.svrepdesign(des)
## calculate island/year means and SE
# standard design
isl_yearmean = svyby(~CORAL, ~ISLAND + ANALYSIS_YEAR, design = des, svymean)
# replicate-weights design
# isl_yearmean = svyby(~CORAL, ~ISLAND + ANALYSIS_YEAR, design = repdes, svymean)
# plot weighted mean coral cover by island and year
ggplot(isl_yearmean, aes(x = ANALYSIS_YEAR, y = CORAL, fill = ANALYSIS_YEAR)) +
geom_bar(stat = "identity", position = position_dodge2(preserve = "single"), width = 1, color = "black") +
geom_errorbar(aes(ymin = CORAL - se, ymax = CORAL + se), width = 0.2) +
facet_wrap(~ISLAND, nrow = 1) +
guides(fill = "none") +
theme_bw() +
theme(panel.grid.minor = element_blank(),
panel.grid.major = element_blank(),
axis.title.x = element_blank(),
axis.title.y = element_blank(),
axis.text.x = element_text(angle = -90, hjust = 0))
# fit Gaussian model
mod.1 = svyglm(CORAL ~ ISLAND * ANALYSIS_YEAR, design = des)
summary(mod.1)
#Test for significance of your fixed effects
car::Anova(mod.1, type = 3, test.statistic = "F")
# fit binomial model
mod.2 = svyglm(CORAL ~ ISLAND + ANALYSIS_YEAR, design = des, family = "binomial") # you will get an warning because it's expecting a 0/1, but it's ok the math will still work out
mod.3 = svyglm(CORAL ~ ISLAND * ANALYSIS_YEAR, design = des, family = "binomial")
# compare models (ignore eff.p and deltabar- telling you how AIC was adjusted)
AIC(mod.2, mod.3)
#guidance on AIC is that you should be using a combination of significance of predictors/likelihood ratio tests and AIC, use AIC carefully by itself
car::Anova(mod.1, type = 3, test.statistic = "F")
# ------------------------------------------------------------------------------------------------------------
# residual diagnostics
# Residuals tells you the difference between what the value was vs. what the model told you it was
# standardize by dividing by SD so you can identify certain points that were of concern
#resids = svydiags::svystdres(mod.3, stvar = "STRAT_CONC")$stdresids
resids = scale(mod.3$residuals)
resids = (mod.3$residuals - mean(mod.3$residuals))/sd(mod.3$residuals)
#Should we be concerned about this plot?
#In standard normal model, residuals should fall withing 2SD of mean
#Kyle, explain why we we should plot fitted residuals rather than use straight up plot(resids)
#When working with survey design don't use plot(mod.3)
plot(fitted(mod.3), resids)
hist(resids)
#
# VIF of min and max depth
1 / (1 - with(mhi, cor(new_MIN_DEPTH_M, new_MAX_DEPTH_M, use = "pairwise.complete.obs"))^2)
# highly correlated; would only include one in model
# plot max depth vs. proportion coral cover
ggplot(mhi, aes(x = new_MAX_DEPTH_M, y = CORAL)) + geom_point() + geom_smooth() + theme_bw()
# no evidence of non-linear trend, but let's add it to the model
# add max depth as linear term
mod.3.1 = svyglm(CORAL ~ ISLAND * ANALYSIS_YEAR + new_MAX_DEPTH_M, design = des, family = "binomial")
summary(mod.3.1)
# add max depth as 2nd degree polynomial
mod.3.2 = svyglm(CORAL ~ ISLAND * ANALYSIS_YEAR + poly(new_MAX_DEPTH_M, 2, raw = TRUE), design = des, family = "binomial")
# compare by AIC
AIC(mod.3.1, mod.3.2)
summary(mod.3.2)
# evidence of a non-linear trend in presence of other covariates that was not apparent in simple 2D plot
# increase to a 3rd degree polynomial
mod.3.3 = svyglm(CORAL ~ ISLAND * ANALYSIS_YEAR + poly(new_MAX_DEPTH_M, 3, raw = TRUE), design = des, family = "binomial")
# compare by AIC
AIC(mod.3.2, mod.3.3)
# no evidence of improved fit with 3rd degree-- we settle on 2nd degree polynomial
# summarize
anova(mod.3.2)
car::Anova(mod.3.2, type = 3, test.statistic = "F")
t(sapply(attr(mod.3.2$terms, "term.labels"), function(x) regTermTest(mod.3.2, x, method = "WorkingWald")[c("df", "ddf", "p")]))
# pseudo R-squared
jtools::summ(mod.3.2)
# planned comparisons
#Testing 2019 vs. 2010-12 in Kauai
#need to identify the reference levels for each categorical list
contr = multcomp::glht(mod.3.2, linfct = c("ANALYSIS_YEAR2019 = 0",
"ANALYSIS_YEAR2019 + ISLANDMaui:ANALYSIS_YEAR2019 = 0",
"ANALYSIS_YEAR2019 + ISLANDKauai:ANALYSIS_YEAR2019 = 0"))
summary(contr)
round(100 * (exp(confint(contr)$confint) - 1), 1)
6.5-2.3
4.2/6.5
86/336
4*10
210*40
/10
8400/10
50*210
9*11
23/3
23/4
69/4
213814+168442+181204
23.72/13.99
23.72-13.99
9.73/13.99
9.73/23.72
rm(list=ls())
library(gdata)             # needed for drop_levels()
library(reshape)           # reshape library inclues the cast() function used below
library(RODBC)            # to connect to oracle
#LOAD LIBRARY FUNCTIONS ...
source("C:/Users/Courtney.S.Couch/Documents/GitHub/Benthic-Scripts/Functions/Benthic_Functions_newApp.R")
#This script combines all historical raw CPC and CoralNet annotations, generates analysis ready data,
#generates site-level % cover for all sites then generates strata and weighed means at the sector and island-level
#in v3, we've made several updates to the pooling scheme and added in the 2022 data.
#Note- CRED/CREP/ESD made the switch from CPC to CoralNet in 2015, but some of the legacy 2012 imagery was analyzed in CoralNet
rm(list=ls())
library(gdata)             # needed for drop_levels()
library(reshape)           # reshape library inclues the cast() function used below
library(RODBC)            # to connect to oracle
#LOAD LIBRARY FUNCTIONS ...
source("C:/Users/Courtney.S.Couch/Documents/GitHub/Benthic-Scripts/Functions/Benthic_Functions_newApp_vTAOfork.R")
source("C:/Users/Courtney.S.Couch/Documents/GitHub/fish-paste/lib/core_functions.R")
source("C:/Users/Courtney.S.Couch/Documents/GitHub/fish-paste/lib/fish_team_functions.R")
source("C:/Users/Courtney.S.Couch/Documents/GitHub/fish-paste/lib/Islandwide Mean&Variance Functions.R")
#Climate data - this is from CPCE
load("T:/Benthic/Data/REA Coral Demography & Cover/Raw from Oracle/ALL_BIA_CLIMATE_PERM.rdata")   #bia
cli$SITE<-SiteNumLeadingZeros(cli$SITE)
#BIA data - this is from CPCE
load("T:/Benthic/Data/REA Coral Demography & Cover/Raw from Oracle/ALL_BIA_STR_RAW_NEW.rdata")   #bia
bia$SITE<-SiteNumLeadingZeros(bia$SITE)
#CNET data - from CoralNet
#These data contain human annotated data. There may be a small subset of robot annotated data.
#The robot annotations are included because the confidence threshold in CoralNet was set to 70-90% allowing the robot to annotate points when it was 70-90% certain.
#2019 NWHI data not in these view because it was analyzed as part of a bleaching dataset
load("T:/Benthic/Data/REA Coral Demography & Cover/Raw from Oracle/ALL_CNET_Annotations.rdata") #load data
cnet<-select(cnet,-c("TYPE"))
head(cnet)
cnet$SITE<-as.factor(cnet$SITE)
cnet$SITE<-SiteNumLeadingZeros(cnet$SITE)
#Read in survey master and sector files
#sm<-read.csv("C:/Users/Courtney.S.Couch/Documents/GitHub/fish-paste/data/SURVEY MASTER.csv")
sm<-read.csv("C:/Users/Courtney.S.Couch/Documents/GitHub/Benthic-Scripts/Survey Master Prep/SURVEY_MASTER_w2013benthic.csv")
sm<-read.csv("C:/Users/Courtney.S.Couch/Documents/GitHub/Benthic-Scripts/Survey Master Prep/SURVEY_MASTER.csv")
#Read in survey master and sector files
#sm<-read.csv("C:/Users/Courtney.S.Couch/Documents/GitHub/fish-paste/data/SURVEY MASTER.csv")
sm<-read.csv("C:/Users/Courtney.S.Couch/Documents/GitHub/Benthic-Scripts/Survey Master Prep/SURVEY_MASTER.csv")
sm<-read.csv("C:/Users/Courtney.S.Couch/Documents/GitHub/fish-paste/data/SURVEY MASTER.csv")
sectors<-read.csv("C:/Users/Courtney.S.Couch/Documents/GitHub/fish-paste/data/Sectors-Strata-Areas.csv")
#Temporary work around for merging in 2014-2017 NWHI data that hasn't been uploaded to Oracle yet- remove this once Michael has incorporated data
new.nw<-read.csv("T:/Benthic/Data/REA Coral Demography & Cover/Raw Data from CoralNet/2014-2017_NWHI_CnetAnnotations_formatted.csv")
new.nw<-new.nw %>% drop_na(ROUNDID) #remove blank rows
new.cnet<-new.nw
class(new.cnet$DATE_)
class(new.cnet$DATE_TAKEN)
#Date conversations still not working
new.cnet$DATE_<-lubridate::mdy(new.cnet$DATE_)
new.cnet$DATE_TAKEN<-lubridate::ymd(new.cnet$DATE_TAKEN);head(new.cnet$DATE_TAKEN)
new.cnet$DATE_ANNOTATED<-lubridate::ymd_hms(new.cnet$DATE_ANNOTATED);head(new.cnet$DATE_ANNOTATED)
#########PLACEHOLDER Temporary work around for merging in 2023 SWAINS data
new.nw<-read.csv("T:/Benthic/Data/REA Coral Demography & Cover/Raw Data from CoralNet/2014-2017_NWHI_CnetAnnotations_formatted.csv")
new.nw<-new.nw %>% drop_na(ROUNDID) #remove blank rows
new.cnet<-new.nw
class(new.cnet$DATE_)
class(new.cnet$DATE_TAKEN)
#Date conversations still not working
new.cnet$DATE_<-lubridate::mdy(new.cnet$DATE_)
new.cnet$DATE_TAKEN<-lubridate::ymd(new.cnet$DATE_TAKEN);head(new.cnet$DATE_TAKEN)
new.cnet$DATE_ANNOTATED<-lubridate::ymd_hms(new.cnet$DATE_ANNOTATED);head(new.cnet$DATE_ANNOTATED)
#combine old cnet and 2015 & 2017 nwhi cnet data
cnet<-rbind(cnet,new.cnet)
10*25
1000/250
300/30
25*30
#CREATE ADULT CLEAN ANALYSIS READY DATA----------------------------------------
# This script will clean the raw benthic REA data using method E that comes directly from the new data base application.
#Note- these data represent the revised data structure instituted in November 2018 and 2019. Several recent dead and condition columns were added
#These data only include surveys conducted between 2013-2020
#NOTE: Depth should not be used the in the raw data because the column was deprecated in Oracale and is inconsistent.
#Use depth data from SURVEY MASTER
rm(list=ls())
#Set Run Flags
DEBUG=TRUE
#LOAD LIBRARY FUNCTIONS ...
source("./Functions/Benthic_Functions_newApp_vTAOfork.R")
#Code for running PERMANOVA, SIMPER, and nMDS plots on Swains coral density data
#written by Brittany and modified by Mia for Swains project
#load libraries
library(tidyverse)
#Code for running PERMANOVA, SIMPER, and nMDS plots on Swains coral density data
#written by Brittany and modified by Mia for Swains project
#load libraries
library(tidyverse)
rm(list=ls())
dir = Sys.info()[7]
setwd(paste0("C:/Users/", dir, "/Documents/GitHub/swains/"))
####PREP MULTIVARIATE DATA MATRIX---------------
adults <- read.csv("Data/Swains_sitedata_TAXONCODE_MORPH.csv")
#filter down to columns we care about
dat <- adults %>%
filter(ISLAND == "Swains") %>%
dplyr::select(SITE, TAXONCODE_2, AdColDen, OBS_YEAR, DEPTH_BIN)%>%
mutate_if(is.character,as.factor)
library(tidyverse)
library(readr)
library(vegan)
library(dplyr)
rm(list=ls())
dir = Sys.info()[7]
setwd(paste0("C:/Users/", dir, "/Documents/GitHub/swains/"))
####PREP MULTIVARIATE DATA MATRIX---------------
adults <- read.csv("Data/Swains_sitedata_TAXONCODE_MORPH.csv")
#filter down to columns we care about
dat <- adults %>%
filter(ISLAND == "Swains") %>%
dplyr::select(SITE, TAXONCODE_2, AdColDen, OBS_YEAR, DEPTH_BIN)%>%
mutate_if(is.character,as.factor)
levels(dat$TAXONCODE_2)
#drop taxa with 0 density at Swains, filter by depth bins, and build species matrix per bin
dat_shallow <- dat %>%
filter(AdColDen > "0")%>%
filter(DEPTH_BIN == "Shallow") %>%
spread(key = "TAXONCODE_2", value = "AdColDen")%>%
replace(is.na(.), 0)
#load libraries
library(tidyverse)
library(readr)
library(vegan)
library(dplyr)
rm(list=ls())
dir = Sys.info()[7]
setwd(paste0("C:/Users/", dir, "/Documents/GitHub/swains/"))
####PREP MULTIVARIATE DATA MATRIX---------------
adults <- read.csv("Data/Swains_sitedata_TAXONCODE_MORPH.csv")
#filter down to columns we care about
dat <- adults %>%
filter(ISLAND == "Swains") %>%
dplyr::select(SITE, TAXONCODE_2, AdColDen, OBS_YEAR, DEPTH_BIN)%>%
mutate_if(is.character,as.factor)
levels(dat$TAXONCODE_2)
#drop taxa with 0 density at Swains, filter by depth bins, and build species matrix per bin
dat_shallow <- dat %>%
filter(AdColDen > "0")%>%
filter(DEPTH_BIN == "Shallow") %>%
spread(key = "TAXONCODE_2", value = "AdColDen")%>%
replace(is.na(.), 0)
?spread
library(tidyr)
#Code for running PERMANOVA, SIMPER, and nMDS plots on Swains coral density data
#written by Brittany and modified by Mia for Swains project
#load libraries
library(tidyverse)
library(readr)
library(vegan)
library(dplyr)
library(tidyr)
rm(list=ls())
dir = Sys.info()[7]
setwd(paste0("C:/Users/", dir, "/Documents/GitHub/swains/"))
####PREP MULTIVARIATE DATA MATRIX---------------
adults <- read.csv("Data/Swains_sitedata_TAXONCODE_MORPH.csv")
#filter down to columns we care about
dat <- adults %>%
filter(ISLAND == "Swains") %>%
dplyr::select(SITE, TAXONCODE_2, AdColDen, OBS_YEAR, DEPTH_BIN)%>%
mutate_if(is.character,as.factor)
levels(dat$TAXONCODE_2)
#drop taxa with 0 density at Swains, filter by depth bins, and build species matrix per bin
dat_shallow <- dat %>%
filter(AdColDen > "0")%>%
filter(DEPTH_BIN == "Shallow") %>%
spread(key = "TAXONCODE_2", value = "AdColDen")%>%
replace(is.na(.), 0)
dat_mid <- dat %>%
filter(AdColDen > "0")%>%
filter(DEPTH_BIN == "Mid") %>%
spread(key = "TAXONCODE_2", value = "AdColDen")%>%
replace(is.na(.), 0)
dat_deep <- dat %>%
filter(AdColDen > "0")%>%
filter(DEPTH_BIN == "Deep") %>%
spread(key = "TAXONCODE_2", value = "AdColDen")%>%
replace(is.na(.), 0)
###check out # of sites by year, depth.
tbl_N<- dat %>%
group_by(OBS_YEAR, DEPTH_BIN) %>%
count()%>%
spread(DEPTH_BIN, n)
tbl_N #unbalanced design; 2018 has twice the sample size across all depths as 2015
#create dataframes for (1) coral taxa & (2) driver varaiable
groups <- dplyr::select (dat_shallow, SITE:DEPTH_BIN)
taxa <- dplyr::select(dat_shallow, ACSP_BR:SPIS)
#create a heat map of taxa matrix
range(taxa) #ideally btwn 0-10
taxa <- (taxa)^(1/2) #square root tranformation to get data range between 0-10
taxa_mtrx <- as.matrix(taxa) #convert to matrix
heatmap(taxa_mtrx) #visualize distribution of dataframe
range(taxa) #ideally btwn 0-10
#eliminate rare species
taxa_pa <- decostand(taxa, "pa") #use 'decostand' function to converts matrix to presence/absence per site
taxa_sum <- apply(taxa_pa, 2, sum) #calculate sum per species over columns
sort(taxa_sum) #view taxa prevalence
taxa_reduced <- taxa[, !taxa_sum<2] # removes taxa only seen once.
#OPTIONAL: adding dummy variable column to species maxtrix if needed as can't run PERMANOVA on 'empty' sites
#taxa_reduced$dummy <- min(apply(taxa, 1, function(x) min(x[x>0]))) #set dummy value to lowest non-zero density value in taxa
#PERMANOVA
pmv_shallow <- adonis2(taxa_reduced  ~  OBS_YEAR, data = groups, permutations = 999, method = "bray", by = "terms")
pmv_shallow
#create dataframes for (1) coral taxa & (2) driver varaiable
groups_m <- dplyr::select (dat_mid, SITE:DEPTH_BIN)
taxa_m <- dplyr::select(dat_mid, ACSP_BR:SPIS)
#create a heat map of taxa matrix
taxa_m <- (taxa_m)^(1/2) #square root tranformation to get data range between 0-10
range(taxa_m) #ideally btwn 0-10
##OPTIONAL: eliminate rare species
taxa_pa_m <- decostand(taxa_m, "pa") #use 'decostand' function to converts matrix to presence/absence per site
taxa_mum_m <- apply(taxa_pa_m, 2, sum) #calculate sum per species over columns
sort(taxa_mum_m) #see which taxa are rare
taxa_reduced_m <- taxa_m[, !taxa_mum_m<2] # removes taxa only seen once.
#PERMANOVA
pmv_mid <- adonis2(taxa_reduced_m  ~  OBS_YEAR, data = groups_m, permutations = 999, method = "bray", by = "terms")
pmv_mid
#create dataframes for (1) coral taxa & (2) driver varaiable
groups_d <- dplyr::select (dat_deep, SITE:DEPTH_BIN)
taxa_d <- dplyr::select(dat_deep, ASTS_MD:SPIS)
#create a heat map of taxa matrix
taxa_d <- (taxa_d)^(1/2) #square root tranformation to get data range between 0-10
range(taxa_d) #ideally btwn 0-10
##OPTIONAL: eliminate rare species
taxa_pa_d <- decostand(taxa_d, "pa") #use 'decostand' function to converts matrix to presence/absence per site
taxa_dum_d <- apply(taxa_pa_d, 2, sum) #calculate sum per species over columns
sort(taxa_dum_d) #see which taxa are rare
taxa_reduced_d <- taxa_d[, !taxa_dum_d<2] # removes taxa only seen once.
#PERMANOVA
pmv_deep <- adonis2(taxa_reduced_d  ~  OBS_YEAR, data = groups_d, permutations = 999, method = "bray", by = "terms")
pmv_deep
